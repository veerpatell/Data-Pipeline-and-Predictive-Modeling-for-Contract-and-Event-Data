{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pyodbc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1737559850863
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connection string\n",
        "server = 'techentdec.database.windows.net'\n",
        "database = 'QAECECRM_dec'\n",
        "username = 'dbadmin'\n",
        "password = 'DashTech1234'\n",
        "connection_string = f\"\"\"\n",
        "Driver={{ODBC Driver 17 for SQL Server}};\n",
        "Server={server};\n",
        "Database={database};\n",
        "Uid={username};\n",
        "Pwd={password};\n",
        "Encrypt=yes;\n",
        "TrustServerCertificate=no;\n",
        "Connection Timeout=30;\n",
        "\"\"\"\n",
        "\n",
        "# Connect to the SQL database\n",
        "try:\n",
        "    connection = pyodbc.connect(connection_string)\n",
        "    print(\"Connection successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"Connection failed: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Define table names\n",
        "table_names = [\n",
        "    '[dbo].[Contract]', '[dbo].[ContractArtist]', '[dbo].[BlueCard]', \n",
        "    '[dbo].[LuEventType]', '[dbo].[BlueCardEventDate]'\n",
        "]\n",
        "\n",
        "# Initialize an empty dictionary to store DataFrames\n",
        "dataframes = {}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Connection successful!\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1737559851016
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fetch data from SQL tables\n",
        "for table in table_names:\n",
        "    query = f\"SELECT * FROM {table}\"\n",
        "    try:\n",
        "        dataframes[table] = pd.read_sql(query, connection)\n",
        "        print(f\"Successfully imported data from table: {table}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to import data from table: {table}. Error: {e}\")\n",
        "\n",
        "# Assign DataFrames to variables for readability\n",
        "contract_df = dataframes.get('[dbo].[Contract]', pd.DataFrame())\n",
        "contract_artist_df = dataframes.get('[dbo].[ContractArtist]', pd.DataFrame())\n",
        "blue_card_df = dataframes.get('[dbo].[BlueCard]', pd.DataFrame())\n",
        "event_type_df = dataframes.get('[dbo].[LuEventType]', pd.DataFrame())\n",
        "blue_card_date_df = dataframes.get('[dbo].[BlueCardEventDate]', pd.DataFrame())\n",
        "\n",
        "# Data Cleaning\n",
        "def clean_dataframe(df):\n",
        "    if df.empty:\n",
        "        return df\n",
        "    # Drop duplicates\n",
        "    df = df.drop_duplicates()\n",
        "    # Replace empty strings or NaNs with None\n",
        "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "    # Standardize column names\n",
        "    df.columns = df.columns.str.strip().str.lower()\n",
        "    return df"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Successfully imported data from table: [dbo].[Contract]\nSuccessfully imported data from table: [dbo].[ContractArtist]\nSuccessfully imported data from table: [dbo].[BlueCard]\nSuccessfully imported data from table: [dbo].[LuEventType]\nSuccessfully imported data from table: [dbo].[BlueCardEventDate]\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1737559899705
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Clean all DataFrames\n",
        "contract_df = clean_dataframe(contract_df)\n",
        "contract_artist_df = clean_dataframe(contract_artist_df)\n",
        "blue_card_df = clean_dataframe(blue_card_df)\n",
        "event_type_df = clean_dataframe(event_type_df)\n",
        "blue_card_date_df = clean_dataframe(blue_card_date_df)\n",
        "\n",
        "# Join tables to create a master dataset\n",
        "# Merge BlueCard with EventType\n",
        "bluecard_eventtype = pd.merge(blue_card_df, event_type_df, how='left', left_on='eventtypeid', right_on='eventtypeid')\n",
        "\n",
        "# Merge Contract with BlueCard\n",
        "contract_bluecard = pd.merge(contract_df, bluecard_eventtype, how='left', left_on='bluecardid', right_on='bluecardid')\n",
        "\n",
        "# Merge ContractArtist\n",
        "full_data = pd.merge(contract_bluecard, contract_artist_df, how='left', left_on='contractid', right_on='contractid')\n",
        "\n",
        "# Merge BlueCardEventDate\n",
        "full_data = pd.merge(full_data, blue_card_date_df, how='left', left_on='bluecardid', right_on='bluecardid')\n",
        "\n",
        "# Handle missing values\n",
        "full_data.fillna({'gross': 0}, inplace=True)  # Replace numeric NaNs with 0\n",
        "full_data.fillna('Unknown', inplace=True)  # Replace categorical NaNs with 'Unknown'\n",
        "\n",
        "# Feature engineering: Add new columns if necessary\n",
        "full_data['event_year'] = pd.to_datetime(full_data['createddate'], errors='coerce').dt.year\n",
        "\n",
        "\n",
        "# Save cleaned data to a CSV for future use\n",
        "full_data.to_csv('cleaned_data.csv', index=False)\n",
        "\n",
        "# Preview cleaned data\n",
        "print(full_data.head())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_249447/3996531290.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n  full_data['event_year'] = pd.to_datetime(full_data['createddate'], errors='coerce').dt.year\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "   contractid  agentid_x officeid  contractstatusid lineofbusinessid  gross_x  \\\n0      100304          0     17.0                 3              2.0   1700.0   \n1      101731        419      1.0                 6              3.0    550.0   \n2      103324          0      1.0                 4              3.0   2000.0   \n3      103331        313      1.0                 5              3.0   1500.0   \n4      103526          0      1.0                 3              3.0   1000.0   \n\n  bluecardid leadsourceid contracttypeid isreseller  ... aref programstotal  \\\n0    Unknown          3.0            1.0       True  ...  0.0       Unknown   \n1    Unknown          5.0            1.0       True  ...  0.0       Unknown   \n2    Unknown          5.0            1.0      False  ...  0.0       Unknown   \n3    Unknown          5.0            1.0       True  ...  0.0       Unknown   \n4    Unknown          5.0            1.0      False  ...  0.0       Unknown   \n\n  eventtimereasonid                 createddate createdbyid updateddate  \\\n0           Unknown  2018-06-02 04:17:54.100000         0.0     Unknown   \n1           Unknown  2018-06-02 04:17:53.747000         0.0     Unknown   \n2           Unknown  2018-06-02 04:17:54.100000         0.0     Unknown   \n3           Unknown  2018-06-02 04:17:54.010000         0.0     Unknown   \n4           Unknown  2018-06-02 04:17:53.750000         0.0     Unknown   \n\n  updatedbyid bluecardeventdateid eventdate event_year  \n0     Unknown             Unknown   Unknown     2018.0  \n1     Unknown             Unknown   Unknown     2018.0  \n2     Unknown             Unknown   Unknown     2018.0  \n3     Unknown             Unknown   Unknown     2018.0  \n4     Unknown             Unknown   Unknown     2018.0  \n\n[5 rows x 125 columns]\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1737559955991
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Replace placeholders\n",
        "full_data.replace('Unknown', np.nan, inplace=True)\n",
        "\n",
        "# Drop rows with missing target variable\n",
        "full_data.dropna(subset=['gross_x'], inplace=True)\n",
        "\n",
        "# Identify high and low cardinality categorical columns\n",
        "categorical_columns = full_data.select_dtypes(include=['object']).columns\n",
        "high_cardinality_cols = [col for col in categorical_columns if full_data[col].nunique() > 50]\n",
        "low_cardinality_cols = [col for col in categorical_columns if full_data[col].nunique() <= 50]\n",
        "\n",
        "# Apply label encoding for high-cardinality columns\n",
        "label_encoder = LabelEncoder()\n",
        "for col in high_cardinality_cols:\n",
        "    full_data[col] = label_encoder.fit_transform(full_data[col].astype(str))\n",
        "\n",
        "# Apply one-hot encoding for low-cardinality columns\n",
        "full_data = pd.get_dummies(full_data, columns=low_cardinality_cols, drop_first=True)\n",
        "\n",
        "# Ensure all remaining columns are numeric\n",
        "full_data = full_data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill remaining missing values with 0\n",
        "full_data.fillna(0, inplace=True)\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1737559980299
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace placeholders and drop rows with missing target variable\n",
        "full_data.replace('Unknown', np.nan, inplace=True)\n",
        "full_data.dropna(subset=['gross_x'], inplace=True)\n",
        "\n",
        "# Drop irrelevant or high-cardinality columns\n",
        "columns_to_drop = ['presenteraccountname', 'venuecontactname', 'createdbyid', 'updatedbyid']  # Add other non-essential columns\n",
        "full_data.drop(columns=columns_to_drop, axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "# Frequency encoding for categorical columns\n",
        "def frequency_encode(df, column):\n",
        "    freq = df[column].value_counts()\n",
        "    df[column] = df[column].map(freq)\n",
        "    return df\n",
        "\n",
        "categorical_columns = full_data.select_dtypes(include=['object']).columns\n",
        "for col in categorical_columns:\n",
        "    full_data = frequency_encode(full_data, col)\n",
        "\n",
        "# Fill missing values with 0\n",
        "full_data.fillna(0, inplace=True)\n",
        "\n",
        "# Ensure all remaining columns are numeric\n",
        "full_data = full_data.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Prepare features (X) and target (y)\n",
        "X = full_data.drop(['gross_x'], axis=1, errors='ignore')  # Drop the target variable\n",
        "y = full_data['gross_x']  # Target variable"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1737559981305
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features (X) and target (y)\n",
        "X = full_data.drop(['gross_x'], axis=1, errors='ignore')  # Drop the target variable\n",
        "y = full_data['gross_x']  # Target variable\n",
        "\n",
        "# Split data for ML\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1737559982025
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1737565867518
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
        "    \"SVR\": SVR(kernel='linear'),\n",
        "    \"XGBoost\": XGBRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Evaluate models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    results[name] = {\"MSE\": mse, \"R-squared\": r2}\n",
        "\n",
        "# Print Results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Mean Squared Error: {metrics['MSE']}\")\n",
        "    print(f\"  R-squared: {metrics['R-squared']}\\n\")\n"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1737559777506
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}